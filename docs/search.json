[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The data that we are using for this project is a Kaggle Date Set investigating diabetes and its connection to a number of different quantitative and qualitative features. In total, the data set contains 21 features ranging from lifestyle decisions to income levels that were recorded in a survey performed by the CDC and a target variable of whether the survey respondent has diabetes. The dataset contains 253,680 survey responses and is not balanced.\nOf the 21 available features, 5 were selected to investigate and use in our models. These 5 features include:\n\nBMI: Body Mass Index of the individual (numeric)\nPhysActivity: Indicator of whether the individual has partaken in physical activity in the past 30 days (encoded 0 = no and 1 = yes)\nAnyHealthcare: Indicator of whether the individual has any kind of health care coverage (encoded 0 = no and 1 = yes)\nSmoker: Indicator of whether the individual has smoked at least 100 cigarettes in their life (encoded 0 = no and 1 = yes)\nGenHlth: Response of individual when asked to rate general health on scale of 1-5. (1 = excellent to 5 = poor)\n\nThe goal of this project is to investigate these variables and their relation with out target variable:\n\nDiabetes_binary: Whether or not the individual has diabetes. (encoded 0 = no diabetes and 1 = prediabetic or diabetic)\n\nFirst, we will perform EDA. The purpose of EDA is to learn more about the distribution of the selected features and how features might interact with our target variable and other features in the model.\nAfter investigating these relationships, we will move on to our modelling step and build models that use the 5 features to predict whether or not in individual is diabetic. We will investigate multiple types of models and parameters of these models and compare model performance on a test set of data. Once we find an optimal set of parameters, we produce a champion model which can be used for future prediction."
  },
  {
    "objectID": "EDA.html#data-introduction",
    "href": "EDA.html#data-introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The data that we are using for this project is a Kaggle Date Set investigating diabetes and its connection to a number of different quantitative and qualitative features. In total, the data set contains 21 features ranging from lifestyle decisions to income levels that were recorded in a survey performed by the CDC and a target variable of whether the survey respondent has diabetes. The dataset contains 253,680 survey responses and is not balanced.\nOf the 21 available features, 5 were selected to investigate and use in our models. These 5 features include:\n\nBMI: Body Mass Index of the individual (numeric)\nPhysActivity: Indicator of whether the individual has partaken in physical activity in the past 30 days (encoded 0 = no and 1 = yes)\nAnyHealthcare: Indicator of whether the individual has any kind of health care coverage (encoded 0 = no and 1 = yes)\nSmoker: Indicator of whether the individual has smoked at least 100 cigarettes in their life (encoded 0 = no and 1 = yes)\nGenHlth: Response of individual when asked to rate general health on scale of 1-5. (1 = excellent to 5 = poor)\n\nThe goal of this project is to investigate these variables and their relation with out target variable:\n\nDiabetes_binary: Whether or not the individual has diabetes. (encoded 0 = no diabetes and 1 = prediabetic or diabetic)\n\nFirst, we will perform EDA. The purpose of EDA is to learn more about the distribution of the selected features and how features might interact with our target variable and other features in the model.\nAfter investigating these relationships, we will move on to our modelling step and build models that use the 5 features to predict whether or not in individual is diabetic. We will investigate multiple types of models and parameters of these models and compare model performance on a test set of data. Once we find an optimal set of parameters, we produce a champion model which can be used for future prediction."
  },
  {
    "objectID": "EDA.html#package-imports",
    "href": "EDA.html#package-imports",
    "title": "Exploratory Data Analysis",
    "section": "Package Imports",
    "text": "Package Imports\nFirst, we can import the necessary packages we need.\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "Time to model! In our EDA portion, we provided some information about the data we are modeling as well as went through some of our features of interest that we will be incorporating into our model. The data that we are using for our model is a Kaggle Date Set investigating diabetes and its connection to a number of different quantitative and qualitative features. The dataset contains 253,680 survey responses and is not balanced.\n5 were selected use in our models. These 5 features include:\n\nBMI: Body Mass Index of the individual (numeric)\nPhysActivity: Indicator of whether the individual has partaken in physical activity in the past 30 days (encoded 0 = no and 1 = yes)\nAnyHealthcare: Indicator of whether the individual has any kind of health care coverage (encoded 0 = no and 1 = yes)\nSmoker: Indicator of whether the individual has smoked at least 100 cigarettes in their life (encoded 0 = no and 1 = yes)\nGenHlth: Response of individual when asked to rate general health on scale of 1-5. (1 = excellent to 5 = poor)\n\nThe goal of our model is to predict the value of a target variable:\n\nDiabetes_binary: Whether or not the individual has diabetes. (encoded 0 = no diabetes and 1 = prediabetic or diabetic)\n\nIn the modeling portion, we will look into two specific types of tree models. These models will be further described later on, but include:\n\nClassification Tree\nRandom Forest\n\nBefore fitting the models, we will split our data into a 70/30 training-test split. We will fit each model using a number of different model parameters and use 5-fold cross validation on the training data to choose the best model of each class. Then, we will run the each of the best models from each class on the testing set and compare the results to choose one champion model. We will be using log-loss to evaluate our models and determine which models perform best."
  },
  {
    "objectID": "Modeling.html#packages",
    "href": "Modeling.html#packages",
    "title": "Modeling",
    "section": "",
    "text": "Before we get to writing any functions that we use, let’s make sure that we load in the necessary packages we need. We can also include some handy code that searches for the package and installs it in the event that it is not already installed on the device that is running the code.\nClick here for the EDA Page"
  },
  {
    "objectID": "EDA.html#eda",
    "href": "EDA.html#eda",
    "title": "Exploratory Data Analysis",
    "section": "EDA",
    "text": "EDA\n\nPackage Imports\nFirst, we can import the necessary packages we need. Since we are working in the tidyverse, we will go ahead and load the tidyverse package to our session. We are also going to use the psych package to help out with EDA steps like doing rough distribution checks.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n\n\n\nData Preprocessing\nNow that we have the necessary package loaded to perform EDA on our data, we will move on to loading in and preprocessing our data. There is one main preprocessing step that is needed for this data, and that is changing categorical and binary features to factors. The features that we will be changing to factors are:\n\nPhysActivity: Indicator of whether the individual has partaken in physical activity in the past 30 days (encoded 0 = no and 1 = yes)\nAnyHealthcare: Indicator of whether the individual has any kind of health care coverage (encoded 0 = no and 1 = yes)\nSmoker: Indicator of whether the individual has smoked at least 100 cigarettes in their life (encoded 0 = no and 1 = yes)\n\nWhile GenHealth is categorical in a way (with numeric 1-5 being interpreted as a different category of one’s view of their health), I have decided to maintain this feature as integer due to the ordinal nature of the feature in hopes that the model is able to capture this ordinality. Also for general interpretability, a 1-5 rating is fairly simple to interpret as long as we keep in mind that 1 is the most healthy and 5 is the least.\n\n# Read in data\ndata &lt;- read_csv(file = \"data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Mutate data to create factors for binary variables\nmodel_data &lt;- data |&gt; select(Diabetes_binary, BMI, PhysActivity, AnyHealthcare, Smoker, GenHlth) |&gt;\n  mutate(Diabetes_binary = ifelse(Diabetes_binary == 1, \"Yes\", \"No\")) |&gt;\n  mutate(PhysActivity= ifelse(PhysActivity == 1, \"Yes\", \"No\")) |&gt;\n  mutate(AnyHealthcare = ifelse(AnyHealthcare == 1, \"Yes\", \"No\")) |&gt;\n  mutate(Smoker = ifelse(Smoker == 1, \"Yes\", \"No\")) |&gt;\n  mutate(across(c(\"Diabetes_binary\", \"PhysActivity\", \"AnyHealthcare\", \"Smoker\"), as.factor))\n\nhead(model_data)\n\n# A tibble: 6 × 6\n  Diabetes_binary   BMI PhysActivity AnyHealthcare Smoker GenHlth\n  &lt;fct&gt;           &lt;dbl&gt; &lt;fct&gt;        &lt;fct&gt;         &lt;fct&gt;    &lt;dbl&gt;\n1 No                 40 No           Yes           Yes          5\n2 No                 25 Yes          No            Yes          3\n3 No                 28 No           Yes           No           5\n4 No                 27 Yes          Yes           No           2\n5 No                 24 Yes          Yes           No           2\n6 No                 25 Yes          Yes           Yes          2\n\n\nGreat, the data turned out just as we expected.\n\n\nMissingness and Distribution\nNext, we are going to check our data for missing values to ensure we have a complete dataset and don’t have a need to impute values.\n\n# Describe data using psych decribe to check missingness and realism\ndescribe(model_data)\n\n                 vars      n  mean   sd median trimmed  mad min max range  skew\nDiabetes_binary*    1 253680  1.14 0.35      1    1.05 0.00   1   2     1  2.08\nBMI                 2 253680 28.38 6.61     27   27.68 4.45  12  98    86  2.12\nPhysActivity*       3 253680  1.76 0.43      2    1.82 0.00   1   2     1 -1.20\nAnyHealthcare*      4 253680  1.95 0.22      2    2.00 0.00   1   2     1 -4.18\nSmoker*             5 253680  1.44 0.50      1    1.43 0.00   1   2     1  0.23\nGenHlth             6 253680  2.51 1.07      2    2.45 1.48   1   5     4  0.42\n                 kurtosis   se\nDiabetes_binary*     2.34 0.00\nBMI                 11.00 0.01\nPhysActivity*       -0.57 0.00\nAnyHealthcare*      15.48 0.00\nSmoker*             -1.95 0.00\nGenHlth             -0.38 0.00\n\n\nSince we know that the we are expecting 253,680 samples in our data and n = 253,680 for each of our features, it looks like we have no missing values for any of our features. Additionally, the rough distributions of our numeric variables look to be reasonable. The only thing we might want to note is the max of the BMI feature. A BMI of 98 is extremely high so it is possible we might have outliers here. As for the binary features, we can interpret 1 as No and 2 as Yes since the as.factor() function alphabetically assigns factor levels. Everything looks to be reasonable for these as well, but it is worth noting that this dataset is heavily unbalanced in our target variable, with a mean of 1.14. This could influence model performance down the road as the model will be shown significantly more negatives than positives.\n\n\nCategorical Features\nFor the next step of EDA, we will go ahead and make contingency tables for each of our categorical features in relation to our target variable in an effort to see if we can spot any interactions between each of the categorical features and our target. Since the number of samples is large, it might be easier to interpret these contingency tables if we turn them into proportions.\n\n# Create proportional contingency tables for each categorical feature\nprop.table(table(model_data[, c(\"Diabetes_binary\", \"PhysActivity\")]))\n\n               PhysActivity\nDiabetes_binary         No        Yes\n            No  0.19197808 0.66868890\n            Yes 0.05147824 0.08785478\n\nprop.table(table(model_data[, c(\"Diabetes_binary\", \"AnyHealthcare\")]))\n\n               AnyHealthcare\nDiabetes_binary          No         Yes\n            No  0.043342006 0.817324976\n            Yes 0.005605487 0.133727531\n\nprop.table(table(model_data[, c(\"Diabetes_binary\", \"Smoker\")]))\n\n               Smoker\nDiabetes_binary         No        Yes\n            No  0.48970356 0.37096342\n            Yes 0.06712788 0.07220514\n\n\nThese contingency tables are a little difficult to interpret considering some of the features and our target are fairly unbalanced, so let’s try to investigate this in a slightly different way. We are really interested in seeing whether a larger than expected proportion of a specific class of the feature has diabetes. To help with this, we can investigate this through the lens of conditional probabilities. For a feature to have predictive power, we would expect \\(P(\\text{Diabetes = Yes | Feature = Yes}) \\neq P(\\text{Diabetes = Yes | Feature = No})\\). In other words, the proportion of individuals with diabetes should differ between feature categories. We can investigate these conditional probabilities next.\n\n# Create conditional tables for each categorical feature\nmodel_data |&gt;\n  group_by(PhysActivity) |&gt;\n  summarize(prop_diabetes = mean(Diabetes_binary == \"Yes\"))\n\n# A tibble: 2 × 2\n  PhysActivity prop_diabetes\n  &lt;fct&gt;                &lt;dbl&gt;\n1 No                   0.211\n2 Yes                  0.116\n\nmodel_data |&gt;\n  group_by(AnyHealthcare) |&gt;\n  summarize(prop_diabetes = mean(Diabetes_binary == \"Yes\"))\n\n# A tibble: 2 × 2\n  AnyHealthcare prop_diabetes\n  &lt;fct&gt;                 &lt;dbl&gt;\n1 No                    0.115\n2 Yes                   0.141\n\nmodel_data |&gt;\n  group_by(Smoker) |&gt;\n  summarize(prop_diabetes = mean(Diabetes_binary == \"Yes\"))\n\n# A tibble: 2 × 2\n  Smoker prop_diabetes\n  &lt;fct&gt;          &lt;dbl&gt;\n1 No             0.121\n2 Yes            0.163\n\n\nThese tables tell a more clear story regarding the predictive power of the features at hand. We can note that each of the categorical features seems to show slight differences in conditional probabilities. PhysActivity seems to be the strongest with the conditional probability of those individuals who did not partake in physical activity having diabetes being significantly higher than those that did. Smoker shows the second largest difference with smoking more than 100 cigarettes leading to a higher conditional probability. Finally, healthcare shows a minimal difference in probability with those having access to healthcare being slightly more likely to have diabetes. For the healthcare feature, it is worth noting that this is likely not causality since those who do not have access to healthcare might be living with undiagnosed diabetes or those that have diabetes are more likely to ensure to maintain healthcare in order to maintain treatment of their chronic condition.\n\n\nNumeric Features\nNext, we will look at the two numeric features we are planning to use in our model. For these, we should be able to create some tables and plots that might help us better understand the relationships at play.\nFirst, we will investigate the relationship between BMI and diabetes. First, we can create a table breaking down the summary statistics of BMI by diabetes status. Then, we can use a density plot to see how the density of BMI might differ for those that have diabetes vs. those that do not.\n\n# Summarize BMI based on diabetes status\nmodel_data |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarise(\n    mean_BMI = mean(BMI),\n    sd_BMI = sd(BMI)\n  )\n\n# A tibble: 2 × 3\n  Diabetes_binary mean_BMI sd_BMI\n  &lt;fct&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n1 No                  27.8   6.29\n2 Yes                 31.9   7.36\n\n# Density plot filled by diabetes status\nmodel_data |&gt;\n  ggplot(aes(x = BMI, fill = Diabetes_binary)) +\n  geom_density(alpha = 0.4) +\n  labs(title = \"BMI Density by Diabetes Status\", x = \"BMI\", y = \"Density\")\n\n\n\n\n\n\n\n\nBased on the table, we can see that the mean BMI for those with diabetes is slightly higher than those without. Interestingly, the spread of BMI for those with diabetes also seems to be slightly higher as demonstrated by the higher standard deviation. The density plot confirms these observations as the density plot of those with diabetes appears to be shifted to the right and wider than the density of those that do not have diabetes.\nNext, we will look at our GenHealth feature. We can perform the same process here, except instead of using a density plot, it might be helpful to use a bar chart of the proportions since the feature includes only integer values between 1 and 5.\n\n# Summarize GenHlth based by diabetes status\nmodel_data |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarise(\n    mean_BMI = mean(GenHlth),\n    sd_BMI = sd(GenHlth)\n  )\n\n# A tibble: 2 × 3\n  Diabetes_binary mean_BMI sd_BMI\n  &lt;fct&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n1 No                  2.39   1.02\n2 Yes                 3.29   1.01\n\n# Plot double bar graph of GenHlth by diabetes status\nmodel_data |&gt;\n  ggplot(aes(x = GenHlth, fill = Diabetes_binary)) +\n  geom_bar(position = \"Dodge\") +\n  labs(title = \"GenHlth by Diabetes Status\", x = \"GenHlth\", y = \"Number of Samples\")\n\n\n\n\n\n\n\n\nThis tells us a similar story to BMI, as those with diabetes ranked themselves as less healthy on average than those without diabetes, as shown be the higher mean value for GenHlth. However, it is worth noting that the two groups do not demonstrate the difference in spread that was noted with BMI. The bar graph confirms this as the “peak” of the distribution appears to be shifted to the right.\nFor this feature, it might also be helpful to look more into the conditional probabilities as well. Specifically, it might be interesting to look at \\(P(\\text{Diabetes | GenHelth)}\\). To do this we can use a proportional bar graph.\n\n# Plot bar graph of conditional proportions for each GenHlth\nmodel_data |&gt;\n  ggplot(aes(x = GenHlth, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"GenHlth by Diabetes Status\", x = \"GenHlth\", y = \"Proportion of Samples\")\n\n\n\n\n\n\n\n\nThis shows us more interesting interactions between these two variables as those that rank themselves in poorer general health are significantly more likely to have diabetes than those that do not.\nThat wraps up the EDA portion, so now let’s move on to the models! Fisrt, since we made some changes to the dataset, we will go ahead and save it as an RDS file, so we can easily access it in our modeling notebook.\n\n# Save as an RDS\nsaveRDS(model_data, file = \"data/model_data.RDS\")"
  },
  {
    "objectID": "EDA.html#link-to-modeling-page",
    "href": "EDA.html#link-to-modeling-page",
    "title": "Exploratory Data Analysis",
    "section": "Link to Modeling Page",
    "text": "Link to Modeling Page\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html#modeling-introduction",
    "href": "Modeling.html#modeling-introduction",
    "title": "Modeling",
    "section": "",
    "text": "Time to model! In our EDA portion, we provided some information about the data we are modeling as well as went through some of our features of interest that we will be incorporating into our model. The data that we are using for our model is a Kaggle Date Set investigating diabetes and its connection to a number of different quantitative and qualitative features. The dataset contains 253,680 survey responses and is not balanced.\n5 were selected use in our models. These 5 features include:\n\nBMI: Body Mass Index of the individual (numeric)\nPhysActivity: Indicator of whether the individual has partaken in physical activity in the past 30 days (encoded 0 = no and 1 = yes)\nAnyHealthcare: Indicator of whether the individual has any kind of health care coverage (encoded 0 = no and 1 = yes)\nSmoker: Indicator of whether the individual has smoked at least 100 cigarettes in their life (encoded 0 = no and 1 = yes)\nGenHlth: Response of individual when asked to rate general health on scale of 1-5. (1 = excellent to 5 = poor)\n\nThe goal of our model is to predict the value of a target variable:\n\nDiabetes_binary: Whether or not the individual has diabetes. (encoded 0 = no diabetes and 1 = prediabetic or diabetic)\n\nIn the modeling portion, we will look into two specific types of tree models. These models will be further described later on, but include:\n\nClassification Tree\nRandom Forest\n\nBefore fitting the models, we will split our data into a 70/30 training-test split. We will fit each model using a number of different model parameters and use 5-fold cross validation on the training data to choose the best model of each class. Then, we will run the each of the best models from each class on the testing set and compare the results to choose one champion model. We will be using log-loss to evaluate our models and determine which models perform best."
  },
  {
    "objectID": "Modeling.html#modeling",
    "href": "Modeling.html#modeling",
    "title": "Modeling",
    "section": "Modeling",
    "text": "Modeling\n\nPackages and Data Load\nFirst, let’s make sure that we load in the necessary packages we need. We will be using the tidymodels framework to model the data so we will go ahead and load in tidyverse and tidymodels. Additionally, we will add in the ranger library for our random forest engine and future to allow for multithreading. We will also go ahead and load in the RDS file containing the data from the EDA portion. Also, we will need to make one adjustment to our data. Since tidymodels treats the first factor of the target variable as “positive”, it will be helpful for interpretation if we ensure our “Yes” value is our first factor.\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.10     ✔ recipes      1.3.1 \n✔ dials        1.4.2      ✔ rsample      1.3.1 \n✔ dplyr        1.1.4      ✔ tailor       0.1.0 \n✔ ggplot2      3.5.2      ✔ tidyr        1.3.1 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ purrr        1.2.0      ✔ yardstick    1.3.2 \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ readr     2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ranger)\nlibrary(future) #for multithread (thanks Norman!)\nplan(multisession, workers = 10)\n\nmodel_data &lt;- read_rds(\"data/model_data.RDS\")\n\n# Ensure yes is the first level of our target factor\nmodel_data &lt;- model_data |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary, levels = c(\"Yes\", \"No\")))\n\n# Check levels\nlevels(model_data$Diabetes_binary)\n\n[1] \"Yes\" \"No\" \n\n# Check tibble\nhead(model_data)\n\n# A tibble: 6 × 6\n  Diabetes_binary   BMI PhysActivity AnyHealthcare Smoker GenHlth\n  &lt;fct&gt;           &lt;dbl&gt; &lt;fct&gt;        &lt;fct&gt;         &lt;fct&gt;    &lt;dbl&gt;\n1 No                 40 No           Yes           Yes          5\n2 No                 25 Yes          No            Yes          3\n3 No                 28 No           Yes           No           5\n4 No                 27 Yes          Yes           No           2\n5 No                 24 Yes          Yes           No           2\n6 No                 25 Yes          Yes           Yes          2\n\n\nGreat, it looks like our data was loaded in successfully.\n\n\nPreparing the Data for Modeling\nBefore we can model the data, we need to go ahead and split the data into our testing and training sets. Since this split is random, we will go ahead and set a seed so our results are reproducible.\n\nset.seed(42)\n\n# Split the data and save each split to tibble\ndata_split &lt;- initial_split(model_data, prop = 0.70)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n# Check split to ensure that it follows specified proportion\nifelse(nrow(train_data) == nrow(model_data) * 0.70, TRUE, FALSE)\n\n[1] TRUE\n\n\nBased on our required proportion check, it appears our data was split correctly. Next, we will go ahead and set up our 5-fold CV into our training set.\n\n# Set up 5-fold validation\ntrain_5_fold &lt;- vfold_cv(train_data, 5)\n\nNow that we have our cross validation set up, let’s go ahead and prepare our recipe for our model fitting. Since most of our features are fairly simple, our recipe will be as well. Since we are using tree models, we should not need to normalize or create dummies for any of our predictors. Therefore out recipe is simply specifying our model.\n\n# Set up recipe\nrecipe &lt;- recipe(Diabetes_binary ~ ., data = train_data)\n\n\n\nClassification Tree\nA classification tree is the simplest tree based model that uses one tree to produce a prediction. The overall goal of the tree is to split up the predictor space into regions with each region producing a prediction. In the case of classification, this prediction is simply the most prevalent class in that region. As for how a classification tree splits the predictors space, it is typically fit using recursive binary splitting. This is a greedy algorithm that aims to minimize some form of loss function based on where splits are made.\nThese models benefit from being extremely interpretable as you can directly follow the reasoning that the prediction is made. Additionally, they benefit from being more flexible in their ability to handle non-linearity in relationships. The main downside of classification trees are that we often see a high amount of variability in the results as small changes in the data being fed into the model can vastly change the splits of the tree and they are very prone to overfitting to our training data if they are allowed to become very deep and complex. For this reason, we will use our 5-fold cross validation to tune the cost-complexity parameter to ensure our model is not overfitting.\nNow that we have an understanding of classification trees, we can use tidymodels to set up our model and engine. We are going to use our cross validation to tune the cost-complexity of our model, so we will set this to tune and leave the rest to their defaults.\n\n# Classification tree set up\ntree_mod &lt;- decision_tree(cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nNext, we will set up the workflow.\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(recipe) |&gt;\n  add_model(tree_mod)\n\n\n\nRandom Forest\nNext, we can turn our attention to random forest models. Random forest models are an ensemble tree modeling method that have some enhancements that are implemented to combat some of the shortcomings of classification trees. Instead of building a single tree, random forest instead builds a “forest” of trees that have randomness built into how they are trained in an effort to produce variations between each tree structure. In order to determine a final prediction, the result of each tree is averaged, and in the case of classification determined by which group has the most “votes” by each tree. These measures aim to reduce variance and make the tree less prone to overfitting when compared to a individual tree.\nIn random forest models, randomness is introduced into each individual tree fit in two different ways. The first is through bootstrapping from the training data with replacement to get mutiple samples to fit on. Each tree is then fit on a different sample and therefore a different tree is produced. This concept is known as bagging. Additionally, random forest models only use a subset of the predictor space at each split. This is done in an effort to reduce correlation between trees that is typically caused by really strong predictors being used for the initial splits. Since some of the trees will not “see” this predictor at the first split, they are forced to split the data using the remaining predictors. This is a parameter, mtry, that can be adjusted in our model and tuned through cross validation. Both of these come together to generate many different trees that help to reduce the variance of the model at the cost of making the model less interpretable.\nNow that we have an understanding of random forest, we can use tidymodels to set up our model and engine. We are going to use our cross validation to tune the mtry of our model, so we will set this to tune and leave the rest to their defaults.\n\n# Classification random forest set up\nrf_mod &lt;- rand_forest(mtry = tune(), trees = 500) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n\nNext, we will set up the workflow.\n\nrf_wkf &lt;- workflow() |&gt;\n  add_recipe(recipe) |&gt;\n  add_model(rf_mod)\n\n\n\nFitting our CV Folds\nNow that we have our models set up, it is now time to fit them on our CV folds to select the best model for each class. First, we will go ahead and set up a tuning grid for each model. Since we are only tuning one parameter, there isn’t really necessity to set up a grid, but we can tell it how many values we would like to search of that parameter. For the classification tree, we will use 10 since that covers most of the reasonable cost complexities. For our random forest grid, we will use 5 so mtry can search using all possible number of predictors (up to 5 in this case).\n\n# Classification tree tuning grid\ntree_grid &lt;- grid_regular(cost_complexity(), levels = 10)\n# Random forest grid\nrf_grid &lt;- tibble(mtry = 1:5)\n\nNow that we have those set up, let’s fit our models.\n\n# Fitting our classification tree model\ntree_fit &lt;- tree_wkf |&gt;\n  tune_grid(resamples = train_5_fold,\n            grid = tree_grid,\n            metrics = metric_set(accuracy, mn_log_loss))\n\n# Fitting our random forest model\nrf_fit &lt;- rf_wkf |&gt;\n  tune_grid(resamples = train_5_fold,\n            grid = rf_grid,\n            metrics = metric_set(accuracy, mn_log_loss))\n\n\n\nDetermining Best Model from Each Class\nNow that we have fit the models for each class, we can look at our result. Since we are choosing the best model based on log loss, we can take the fit and simply collect our metrics and filter for log loss and sort it.\n\n# Filter metrics for tree fit\ntree_fit |&gt; collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 7\n   cost_complexity .metric     .estimator  mean     n std_err .config         \n             &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1    0.0000000001 mn_log_loss binary     0.354     5 0.00135 pre0_mod01_post0\n 2    0.000000001  mn_log_loss binary     0.354     5 0.00135 pre0_mod02_post0\n 3    0.00000001   mn_log_loss binary     0.354     5 0.00135 pre0_mod03_post0\n 4    0.0000001    mn_log_loss binary     0.354     5 0.00135 pre0_mod04_post0\n 5    0.000001     mn_log_loss binary     0.354     5 0.00135 pre0_mod05_post0\n 6    0.00001      mn_log_loss binary     0.356     5 0.00204 pre0_mod06_post0\n 7    0.001        mn_log_loss binary     0.359     5 0.00179 pre0_mod08_post0\n 8    0.0001       mn_log_loss binary     0.359     5 0.00184 pre0_mod07_post0\n 9    0.01         mn_log_loss binary     0.404     5 0.00206 pre0_mod09_post0\n10    0.1          mn_log_loss binary     0.404     5 0.00206 pre0_mod10_post0\n\n# Filter metrics for rf fit\nrf_fit |&gt; collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n std_err .config        \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1     2 mn_log_loss binary     0.346     5 0.00188 pre0_mod2_post0\n2     3 mn_log_loss binary     0.346     5 0.00199 pre0_mod3_post0\n3     4 mn_log_loss binary     0.348     5 0.00203 pre0_mod4_post0\n4     1 mn_log_loss binary     0.359     5 0.00218 pre0_mod1_post0\n5     5 mn_log_loss binary     0.366     5 0.00272 pre0_mod5_post0\n\n\nFrom our model fit, it appears that the best performing classification tree model had a cost_complexity of 1e-10 and the best random forest model had and mtry of 2. We can now use these parameter values going forward. Now that we found the best parameters, we can go ahead and save them.\n\n# Storing parameters\ntree_best_params &lt;- select_best(tree_fit, metric = \"mn_log_loss\")\nrf_best_params &lt;- select_best(rf_fit, metric = \"mn_log_loss\")\n\nNow, we can finalize our workflow for each class of model to prepare to retrain them on the entirity of the training set and test on the test set.\n\n# Final tree workflow\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\n# Final random forest workflow\nrf_final_wkf &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params)\n\n\n\nComparing the Two Best Models from Each Class\nNow that we have our final workflow for each class, we can refit each of the models on the entire training set so we can compare the two with the test set.\n\n# Fit the final tree\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(accuracy, mn_log_loss))\n# Fit the final random forest\nrf_final_fit &lt;- rf_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(accuracy, mn_log_loss))\n\nSince the model have been fit, we can simply take a look at the metrics and see which performed the best!\n\n# Collect metrics for each model\ntree_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 accuracy    binary         0.861 pre0_mod0_post0\n2 mn_log_loss binary         0.350 pre0_mod0_post0\n\nrf_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 accuracy    binary         0.862 pre0_mod0_post0\n2 mn_log_loss binary         0.345 pre0_mod0_post0\n\n\nBased on the results of our final fit, it appears that the random forest algorithm ended up performing slightly better than the classification tree with a slightly lower log-loss.\nLet’s dive deeper into these results by making a confusion matrix.\n\n# Get the predictions from the final fit\npredictions &lt;- rf_final_fit |&gt; collect_predictions()\n# Create a confusion matrix\nconf_mat(predictions, truth = Diabetes_binary, estimate = .pred_class)\n\n          Truth\nPrediction   Yes    No\n       Yes   394   283\n       No  10207 65220\n\n\nFrom the confusion matrix, we can clearly tell that the model is not near sensitive enough and produces an overwhelming amount of no predictions. This could be due to multiple factors. The two primary causes are likely the unbalanced nature of the data and the lack of a very decisive feature in our feature set.\n\n\nFitting the Model to the Full Dataset\nNow that we have our “champion” model. We can go ahead and fit the model on the entirety of the dataset.\n\n# Fit full random forest model\nrf_full_model &lt;- rf_final_wkf |&gt; fit(model_data)\n\nNow for convenience when setting up the API endpoint, we can save this full model as an RDS.\n\nsaveRDS(rf_full_model, file = \"model/full_model.RDS\")"
  },
  {
    "objectID": "Modeling.html#link-to-eda-page",
    "href": "Modeling.html#link-to-eda-page",
    "title": "Modeling",
    "section": "Link to EDA Page",
    "text": "Link to EDA Page\nClick here for the EDA Page"
  }
]